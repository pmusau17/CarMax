# Spikes are positively correlated every 12 months
Pacf(Train, lag.max=64)  # Two significant spike at lag 1 and 13. AR(2)
nsdiffs(train)           # Probably integrate seasonal component
Acf(diff(BoxCox(Train,lam), 12), lag.max = 64) #Decaying ACF
Pacf(diff(BoxCox(Train,lam), 12), lag.max = 64) #2 maybe 3 spikes -- indicative of AR(2) or AR(3)
# with one integration
summary(ur.kpss(diff(BoxCox(Train,lam), 12), use.lag = 12)) # suggest only 2 spikes above critical value
summary(ur.kpss(diff(BoxCox(Train,lam), 12)))
?ur.kpss
?Pacf
## Question 5
(Smodel<-Arima(Train,order=c(0,0,0),
seasonal=list(order=c(2,1,0),period=12),
lambda = lam))
## Question 5
(Smodel<-Arima(Train,order=c(0,0,0),
seasonal=list(order=c(3,1,0),period=12),
lambda = lam))
(Model1<-Arima(Train,order=c(3,1,0),
seasonal=list(order=c(2,1,0),period=12),
lambda = lam))
(Model1<-Arima(Train,order=c(3,1,0),
seasonal=list(order=c(3,1,0),period=12),
lambda = lam))
## Question 7
(Model2 <- auto.arima(Train, seasonal = T))
(Model3 <- Arima(Train,order=c(3,1,0),
seasonal=list(order=c(2,1,0),period=12)))
(Model3 <- Arima(Train,order=c(3,1,0),
seasonal=list(order=c(3,1,0),period=12)))
(Model3 <- Arima(Train,order=c(3,1,0),
seasonal=list(order=c(2,1,0),period=12)))
(Model3 <- Arima(Train,order=c(3,1,0),
seasonal=list(order=c(2,1,1),period=12)))
(Model3 <- Arima(Train,order=c(3,1,0),
seasonal=list(order=c(2,1,0),period=12)))
(Model3 <- Arima(Train,order=c(3,1,1),
seasonal=list(order=c(2,1,0),period=12)))
(Model3 <- Arima(Train,order=c(3,1,0),
seasonal=list(order=c(2,1,0),period=12)))
(Model3 <- Arima(Train,order=c(3,1,0),
seasonal=list(order=c(2,1,0),period=12),
lambda = lam))
(Model3 <- Arima(Train,order=c(3,1,1),
seasonal=list(order=c(2,1,0),period=12),
lambda = lam))
(Model3 <- Arima(Train,order=c(3,1,1),
seasonal=list(order=c(2,1,1),period=12),
lambda = lam))
(Model3 <- Arima(Train,order=c(2,1,1),
seasonal=list(order=c(2,1,1),period=12),
lambda = lam))
(Model3 <- Arima(Train,order=c(2,1,1),
seasonal=list(order=c(2,1,1),period=12),
lambda = NULL))
(Model3 <- Arima(Train,order=c(2,1,1),
seasonal=list(order=c(2,1,0),period=12),
lambda = NULL))
(Model3 <- Arima(Train,order=c(3,1,1),
seasonal=list(order=c(2,1,0),period=12),
lambda = NULL))
(Model3 <- Arima(Train,order=c(3,1,0),
seasonal=list(order=c(2,1,0),period=12),
lambda = NULL))
auto.arima(Train,stepwise = F,approximation = F)
(Model3 <- Arima(Train,order=c(3,1,0),
seasonal=list(order=c(2,1,0),period=12),
lambda = NULL))
Acf(Smodel$residuals)    #Declining/Decaying Acf
Pacf(Smodel$residuals)   #3 spike in Pacf         possible AR(3)
# addition to seasonal differencing
summary(ur.kpss(diff(BoxCox(Train,lam), 12))) # unit root confirms
summary(ur.kpss(diff(BoxCox(Train,lam), 12)))
Acf(diff(BoxCox(Train,lam), 12), lag.max = 64) #Decaying ACF
Pacf(diff(BoxCox(Train,lam), 12), lag.max = 64) #2 maybe 3 spikes -- indicative of AR(2) or AR(3)
Pacf(diff(diff(BoxCox(Train,lam), 12), lag.max = 64)) #2 maybe 3 spikes -- indicative of AR(2) or AR(3)
Acf(diff(diff(BoxCox(Train,lam), 12), lag.max = 64)) #Decaying ACF
Pacf(diff(diff(BoxCox(Train,lam), 12), lag.max = 64)) #2 maybe 3 spikes -- indicative of AR(2) or AR(3)
Acf(Model3$residuals)    #Declining/Decaying Acf
Pacf(Model3$residuals)
print('White noise')
## Question 17
a<-0.25
ens<-a*forecast1$mean+(1-a)*forecast2$mean
## Question 18
(ME<-mean(Test-ens))                     #Mean Error
Pacf(Model3$residuals)
print('White noise')
## Question 9
forecast1<-forecast::forecast(Model3,h=30)  # h number of test periods ahead
## Question 10
# Assess the accuracy of the model against the test set.
(acc1<-round(accuracy(forecast1,Test),2))  # Pick this model.
Models<-data.frame(acc1[2,1:5])
colnames(Models)<-c("S-ARIMA")
## Question 11
par(mai=c(0.5,0.5,0.5,0.5))
plot(window(Train),type="l",xlim=c(1991.0,2009.0),ylim=c(0.2,1.5),
ylab="Percent Monthly Change",xlab="Period", main="S-ARIMA")
lines(Test,col="black",lwd=2)
lines(forecast1$mean,col="green",lwd=2)
abline(v=2006,lty=2)
legend(2005, 1.5, legend=c("Test", "Forecast"),
col=c("black","green", "red", "red"),
lty=c(1,1,2,2), cex=0.8, text.font=1, bg='lightblue')
plot(forecast1)
## Question 12
Model4 <- ets(Train)
summary(Model4)  # Additive for level, None for slope, Additive for season.
Model4$par[1:3]
## Question 13
forecast2<-forecast::forecast(Model4,h=30)
## Question 14
(acc2<-accuracy(forecast2,Test))
Models <- data.frame(Models,ETS = acc2[2,1:5])
## Question 15
par(mai=c(0.5,0.5,0.5,0.5))
plot(window(Train),type="l",xlim=c(1991.0,2009.0),ylim=c(0.2,1.5),
ylab="Percent Monthly Change",xlab="Period", main="ETS")
lines(Test,col="black",lwd=2)
lines(forecast2$mean,col="green",lwd=2)
abline(v=2006,lty=2)
legend(2005, 1.5, legend=c("Test", "Forecast"),
col=c("black","green"),
lty=c(1,1,2,2), cex=0.8, text.font=1, bg='lightblue')
## Question 16
farima<-function(x,h){
forecast::forecast(Arima(x,order=c(3,1,0),
seasonal=list(order=c(2,1,0),
period=12)),h=h)
}
## Question 17
a<-0.25
ens<-a*forecast1$mean+(1-a)*forecast2$mean
## Question 18
(ME<-mean(Test-ens))                     #Mean Error
(RMSE<-sqrt(mean((Test-ens)^2)))         #Root Mean Squared Error
ens <- ts()
## Question 17
a <- 0.01
ens <- ts()
RMSEvals <- c()
alphas <- c()
alphas <- seq(0.01,0.5,0.01)
## Question 17
rm(a)
alphas <- seq(0.01,0.5,0.01)
RMSEvals <- c()
for (a in alphas) {
ens<-a*forecast1$mean+(1-a)*forecast2$mean
RMSEvals<-c(RMSEvals,sqrt(mean((Test-ens)^2)))
}
min(alphas)
min(RMSEvals)
Results <- data.frame(Alpha = alphas,RMSE = RMSEvals)
Results
glue("The alpha value which minimizes the RMSE against the test set is {Results$print[which.min(Results$RMSE)]} textbooks yields the best expected profit of {round(max(Table$expected_profits),2)} and SD of {round(Table$sd_profits[which.max(Table$expected_profits)],2)}")
glue("The alpha value which minimizes the RMSE against the test set is {Results$print[which.min(Results$RMSE)] which yields an RMSE of {Results$RMSE[which.min(Results$RMSE)]}")
glue("The alpha value which minimizes the RMSE against the test set is {Results$print[which.min(Results$RMSE)]} which yields an RMSE of {Results$RMSE[which.min(Results$RMSE)]}")
glue("The alpha value which minimizes the RMSE against the test set is {Results$Alpha[which.min(Results$RMSE)]} which yields an RMSE of {Results$RMSE[which.min(Results$RMSE)]}")
min(RMSEvals)
a<-min(Results$Alpha)
ens<-a*forecast1$mean+(1-a)*forecast2$mean
## Question 18
(ME<-mean(Test-ens))                     #Mean Error
(RMSE<-sqrt(mean((Test-ens)^2)))         #Root Mean Squared Error
(MAE<-mean(abs(Test-ens)))               #Mean Absolute Error
(MPE<-mean((Test-ens)/Test)*100)         #Mean Percent Error
(MAPE<-mean(abs((Test-ens)/Test))*100)   #Mean Absolute Percentage
ensemble<-c(ME,RMSE,MAE,MPE,MAPE)
Models<-data.frame(Models,ensemble)
View(Models)
View(Results)
a<-Results$Alpha[which.min(Results$RMSE)]
ens<-a*forecast1$mean+(1-a)*forecast2$mean
## Question 18
(ME<-mean(Test-ens))                     #Mean Error
(RMSE<-sqrt(mean((Test-ens)^2)))         #Root Mean Squared Error
(MAE<-mean(abs(Test-ens)))               #Mean Absolute Error
(MPE<-mean((Test-ens)/Test)*100)         #Mean Percent Error
(MAPE<-mean(abs((Test-ens)/Test))*100)   #Mean Absolute Percentage
ensemble<-c(ME,RMSE,MAE,MPE,MAPE)
Models<-data.frame(Models,ensemble)
View(Models)
rm(list = ls())
#Honor Code
print("As a member of the William and Mary community, I pledge on my honor not to lie, cheat, or steal, either in my academic or personal life. I understand that such acts violate the Honor Code and undermine the community of trust, of which we are all stewards.")
library(extraDistr)
library(glue)
################################ Simulation ###################################
## Question 1 ##
fixed_unit_cap_cost<-16   # The fixed cost of producing one compound
sell_price<- 5.75         # Price at which we can sell the compound
variable_cost<- 0.52      # The variable cost per compound
discount_rate <- 0.08     # Discount rate
nrep <- 10                # Number of years
## Question 2 ##
npv <- function(x, r, t0=FALSE){
r <- rep(r, length(x))
sum(x/cumprod(1+r))
}
# Simulation
expected_profits<-c()
sd_profits<-c()
NPV <- c()
s <- 100000
caps <- seq(30e3L,50e3L,2000)
for (i in caps){
capacity <- i
fixed_cost <- capacity*fixed_unit_cap_cost
profit <- c()
for (j in 1:s){
demand <- rnorm(nrep,mean = 40e3L, sd = 10e3L)
RevMinusVC <- c()
for (k in 1:nrep){
RevMinusVC[k] <- min(capacity,demand[k])*(sell_price-variable_cost)
}
NPV<- npv(RevMinusVC,discount_rate)
profit<- c(profit, NPV - fixed_cost)#- fixed_cost
}
expected_profits <- c(expected_profits,mean(profit))
sd_profits <- c(sd_profits,sd(profit))
}
(Table <- data.frame(caps,expected_profits))
#  caps expected_profits
# 1  30000         543504.1
# 2  32000         568879.7
# 3  34000         589988.0
# 4  36000         606743.1
# 5  38000         618021.5
# 6  40000         623689.5
# 7  42000         623954.3
# 8  44000         619008.4
# 9  46000         608475.4
# 10 48000         593184.8
# 11 50000         574887.8
glue("The capacity level that maximizes Vought International's expected profit of ${max(Table$expected_profits)} is {Table$caps[which.max(Table$expected_profits)]} units.")
## Question 3 ##
# Generate Plot to show convergence
plot(Table, type = 'l',xlab = 'Capacity Size',
ylab = 'Expected Profit ($)', main = 'Vought International Expected Earnings by Compound-V Capacity',
col = "blue", pch = 2, xaxt ='n')
axis(1,caps)
points(x=Table$caps, y=Table$expected_profits, cex = 1, col = "dark red", pch = 3)
points(x=Table$caps[which.max(Table$expected_profits)],y=max(Table$expected_profits),cex =2, col="green",pch=8)
clip(Table$caps[which.max(Table$expected_profits)], Table$caps[which.max(Table$expected_profits)], 0, max(Table$expected_profits))
abline(v=Table$caps[which.max(Table$expected_profits)],lty=2)
################################ TimeSeries ###################################
## Question 1
library(urca)
library(forecast)
library(fpp2)
Data <- h02
## Question 2
Train<-window(Data,start=c(1991,7),end = c(2005,12))
Test <-window(Data,start=c(2006,1),end = c(2008,6))
## Question 3
plot(Train)       # Seems to drift upwards and is not stationary.
# The mean and is not time invariant as it is
# slowly trending upwards, changing the value of the mean.
# However, if the mean is changing by at a constant rate, then
# the variance is time-invariant Var(X + c) = Var(X).
# This is hard to determine from the graph.
(var(Train))      # 0.04719823
(var(Test))       # 0.04767388. Very close values
frequency(Train)  # Looks like there is a seasonal component to the train data
# S = 12
nsdiffs(Train)    #Confirms the presence of seasonality - differences needed
ndiffs(Train)
summary(ur.kpss(Train)) # unit root present. Therefore, confirms non-stationary
lam <- BoxCox.lambda(Train)
plot(diff(BoxCox(Train,lam), 12))
ndiffs(diff(BoxCox(Train,lam), 12)) # Seems regular differencing is required in
# addition to seasonal differencing
summary(ur.kpss(diff(BoxCox(Train,lam), 12))) # unit root confirms
plot(diff(diff(BoxCox(Train,lam), 12))) ## looks stationary after transformation
# and differencing
summary(ur.kpss(diff(diff(BoxCox(Train,lam), 12)))) # No unit root. Null hypothesis
## Question 4
Acf(Train, lag.max = 64) # Slow decaying ACF
# Spikes are positively correlated every 12 months
Pacf(Train, lag.max=64)  # Three significant spike at lag 1, 11 and 12. AR(3)
nsdiffs(train)           # Probably integrate seasonal component
Acf(diff(diff(BoxCox(Train,lam), 12), lag.max = 64)) #Decaying ACF
Pacf(diff(diff(BoxCox(Train,lam), 12), lag.max = 64)) #2 maybe spikes -- indicative of S-AR(2)
# with one integration
# suggest only 2 spikes above critical value
print("I recommend an AR(2) with 1 seasonal integration (differencing).")
## Question 5
(Smodel<-Arima(Train,order=c(0,0,0),
seasonal=list(order=c(2,1,0),period=12),
lambda = lam))
Acf(Smodel$residuals, lag.max = 64)    # Quickly Declining/Decaying Acf
Pacf(Smodel$residuals, lag.max = 64)   #3 spike in Pacf; possible AR(3)
## Question 6
print("In the ACF plot of the residuals, we see a decaying graph. The PACF
graph shows three spikes. This suggests an AR(3) model.")
(Model1<-Arima(Train,order=c(3,1,0),
seasonal=list(order=c(2,1,0),period=12),
lambda = lam))
## Question 7
(Model2 <- auto.arima(Train, seasonal = T))
(Model3 <- Arima(Train,order=c(3,1,0),
seasonal=list(order=c(2,1,0),period=12),
lambda = NULL))
## Question 8
Acf(Model3$residuals, lag.max = 64)
Pacf(Model3$residuals, lag.max = 64)
Box.test((Model3$residuals), lag=64, type="Ljung-Box")
print('We see there is one spike of the critical values in both graphs. However, this might be and outlier. The remaining evidence and Ljung-Box test p-value suggest White noise.')
## Question 9
forecast1<-forecast::forecast(Model3,h=30)  # h number of test periods ahead
## Question 10
# Assess the accuracy of the model against the test set.
(acc1<-round(accuracy(forecast1,Test),2))  # Pick this model.
Models<-data.frame(acc1[2,1:5])
colnames(Models)<-c("S-ARIMA")
## Question 11
par(mai=c(0.5,0.5,0.5,0.5))
plot(window(Train),type="l",xlim=c(1991.0,2009.0),ylim=c(0.2,1.5),
ylab="Percent Monthly Change",xlab="Period", main="S-ARIMA")
lines(Test,col="black",lwd=2)
lines(forecast1$mean,col="green",lwd=2)
abline(v=2006,lty=2)
legend(2005, 1.5, legend=c("Test", "Forecast"),
col=c("black","green", "red", "red"),
lty=c(1,1,2,2), cex=0.8, text.font=1, bg='lightblue')
## Question 12
Model4 <- ets(Train)
summary(Model4)  # Additive for level, None for slope, Additive for season.
print(Model4$par[1:3])
## Question 13
forecast2<-forecast::forecast(Model4,h=30)
## Question 14
(acc2<-accuracy(forecast2,Test))
Models <- data.frame(Models,ETS = acc2[2,1:5])
## Question 15
par(mai=c(0.5,0.5,0.5,0.5))
plot(window(Train),type="l",xlim=c(1991.0,2009.0),ylim=c(0.2,1.5),
ylab="Percent Monthly Change",xlab="Period", main="ETS")
lines(Test,col="black",lwd=2)
lines(forecast2$mean,col="green",lwd=2)
abline(v=2006,lty=2)
legend(2005, 1.5, legend=c("Test", "Forecast"),
col=c("black","green"),
lty=c(1,1,2,2), cex=0.8, text.font=1, bg='lightblue')
## Question 16
farima<-function(x,h){
forecast::forecast(Arima(x,order=c(3,1,0),
seasonal=list(order=c(2,1,0),
period=12)),h=h)
}
e1<-tsCV(Train,farima,h=1)
(cv1<-sqrt(mean(e1^2,na.rm=TRUE)))
fets<-function(x,h){
forecast::forecast(ets(x),h=h)
}
e2<-tsCV(Train,fets,h=1)
(cv2<-sqrt(mean(e2^2,na.rm=TRUE)))
glue("After LOOCV, the RMSE of S-ARIMA is {cv1} and the RMSE of ETS is {cv2}")
print("Best model is ETS because of overall consistency in having lower error metrics, despite S-ARIMA have a lower LOOCV RMSE.")
## Question 17
rm(a)
ens <- ts()
alphas <- seq(0.01,0.99,0.01)
RMSEvals <- c()
for (a in alphas) {
ens<-a*forecast1$mean+(1-a)*forecast2$mean
RMSEvals<-c(RMSEvals,sqrt(mean((Test-ens)^2)))
}
Results <- data.frame(Alpha = alphas,RMSE = RMSEvals)
glue("The alpha value which minimizes the RMSE against the test set is {Results$Alpha[which.min(Results$RMSE)]} which yields an RMSE of {Results$RMSE[which.min(Results$RMSE)]}")
print("This RMSE is lower than the RMSE values of the ETS and S-ARIMA model independently.")
a<-Results$Alpha[which.min(Results$RMSE)]
ens<-a*forecast1$mean+(1-a)*forecast2$mean
## Question 18
(ME<-mean(Test-ens))                     #Mean Error
(RMSE<-sqrt(mean((Test-ens)^2)))         #Root Mean Squared Error
(MAE<-mean(abs(Test-ens)))               #Mean Absolute Error
(MPE<-mean((Test-ens)/Test)*100)         #Mean Percent Error
(MAPE<-mean(abs((Test-ens)/Test))*100)   #Mean Absolute Percentage
ensemble<-c(ME,RMSE,MAE,MPE,MAPE)
Models<-data.frame(Models,ensemble)
# Plot
par(mai=c(0.5,0.5,0.5,0.5))
plot(window(Train),type="l",xlim=c(1991.0,2009.0),ylim=c(0.2,1.5),
ylab="Percent Monthly Change",xlab="Period", main="Ensemble")
lines(Test,col="black",lwd=2)
lines(ens,col="green",lwd=2)
abline(v=2006,lty=2)
legend(2005, 1.5, legend=c("Test", "Forecast"),
col=c("black","green"),
lty=c(1,1,2,2), cex=0.8, text.font=1, bg='lightblue')
print("Based on the accuracy of the three different model, I recommend using the ensemble as it, overall, has the lowest error metrics.")
print("We also see from the graph that it has the best fit compared to the test set.")
?haversine
4535*46
4535*46*2
## clear global environment
rm(list=ls())
#install appropriate libraries
library(tidyverse)
library(broom)
library(ggplot2)
library(caret)
library(car)
CovidLR <- read.csv("~/CovidLR.csv", stringsAsFactors = TRUE)
## remove all NA's from the dataset
CovidLR <- na.omit(CovidLR)
# Commands to get to know the dataset
str(CovidLR) ## all the 13 varaibles in the dataset are categorical. Therefore,
summary(CovidLR)
#view all boxplots to get an idea of data spread and how it relates to the
#outcome od died variable
modelOverall <- glm(died ~ ., data = CovidLR, family = binomial)
probabilities <- predict(modelOverall, CovidLR, type='response')
predictors <- ifelse(probabilities > 0.5, "Yes","No")
categoricalPredictors <- CovidLR[,-1]
preds <- colnames(categoricalPredictors)
categoricalPredictors <- categoricalPredictors %>%
mutate(logit = log(probabilities/(1-probabilities))) %>%
gather(key="preds", value = "pred.value", - logit)
knitr::opts_chunk$set(echo = TRUE)
```{r, eval = F}
Autocorrelation use BG test or correlation of the error term?
library(ISLR)
data("Default")
attach(Default)
str(Default)
library(ggplot2)
ggplot(Default, aes(balance, income, group = default)) + geom_point(aes(shape = default, color = default))
install.packages("caret")
library(caret)
#list = false is so that it unlists the data for us
divideData <- createDataPartition(Default$default, p = 0.8, list = FALSE)
train <- Default[divideData,]
str(train)
boxplot(Default$balance , Default$default, col = rainbow(2))
test <- Default[-divideData,]
#Logistic Regression
logisticreg <- glm(default~balance, family=binomial, data = train)
summary(logisticreg)
## when summarising, easier to take exp so we can interpret in terms of odds
exp(coef(logisticreg))
library(tidyverse)
#prob not really probability, just assignment
prob <- ifelse(Default$default == "Yes", 1, 0)
Default %>% ggplot(aes(balance, prob)) +
geom_point(alpha = .1) +
geom_smooth(method = "glm", method.args = list(family = "binomial")) +
ggtitle("Logistic regression model fit") +
xlab("Balance") + ylab("Probability of Default")
data("PimaIndiansDiabetes2")
attach(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
model <- glm(diabetes~ . , data= PimaIndiansDiabetes2, family = binomial)
summary(model)
probabilities <- predict(model, PimaIndiansDiabetes2, type='response')
pred <- ifelse(probabilities > 0.5, "pos","neg")
table(pred,PimaIndiansDiabetes2[2])
## Test individual x's for linearity with their log self
Linear <- glm(diabetes~age*log(age), data = PimaIndiansDiabetes2, family = binomial)
summary(Linear)
numericalData <- select_if(PimaIndiansDiabetes2, is.numeric)
predictors <- colnames(numericalData)
numericalData <- numericalData %>%
mutate(logit = log(probabilities/(1-probabilities))) %>%
gather(key="predictors", value = "predictor.value", - logit)
library(mlbench)
data("PimaIndiansDiabetes2")
attach(PimaIndiansDiabetes2)
summary(PimaIndiansDiabetes2)
PimaIndiansDiabetes2 <- na.omit(PimaIndiansDiabetes2)
model <- glm(diabetes~ . , data= PimaIndiansDiabetes2, family = binomial)
summary(model)
probabilities <- predict(model, PimaIndiansDiabetes2, type='response')
pred <- ifelse(probabilities > 0.5, "pos","neg")
table(pred,PimaIndiansDiabetes2[2])
## Test individual x's for linearity with their log self
Linear <- glm(diabetes~age*log(age), data = PimaIndiansDiabetes2, family = binomial)
summary(Linear)
numericalData <- select_if(PimaIndiansDiabetes2, is.numeric)
predictors <- colnames(numericalData)
numericalData <- numericalData %>%
mutate(logit = log(probabilities/(1-probabilities))) %>%
gather(key="predictors", value = "predictor.value", - logit)
ggplot(numericalData, aes(logit, predictor.value)) + geom_point(size=.5) +
geom_smooth() + facet_wrap(~predictors, scales = "free_y")
View(numericalData)
# Using only the predictors gender, pneumonia and asthma, we see a
# testing error rate of 0.06067678. Thus, the full prediction model with all
1/ 0.01430165
library(boot)
knitr::include_graphics("C:/Users/ttran/Documents/ML1/bootstrap.png")
knitr::include_graphics("C:/Users/ttran/Documents/ML1/bootstrap.png")
# Question 13
library(ISLR)
data("Hitters")
lm2 <- lm(Salary~AtBat+Hits+Walks+CRuns+PutOuts+CWalks, data=Hitters)
plot(lm2, which = c(1))
car::outlierTest(lm2)
car::vif(lm2)
lmtest::bptest(lm2)
mtcars
?mtcars
Data = read.csv("CaseDataRemodeled.csv",stringsAsFactors = TRUE)
setwd("C:/Users/ttran/Desktop/CarMax")
Data = read.csv("CaseDataRemodeled.csv",stringsAsFactors = TRUE)
# Look that strings are factors
str(Data)
summary(Data)
